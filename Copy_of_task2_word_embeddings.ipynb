{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Copy of task2_word_embeddings.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NosenkoArtem/Neural_Networks_and_CV/blob/master/Copy_of_task2_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnsHq7OydQxI",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqRNEI_6dQxL",
        "colab_type": "code",
        "outputId": "8355d707-2e90-409e-ea18-7ff74330e1d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle,\n",
        "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
        "\n",
        "!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
        "import sys; sys.path.append('./stepik-dl-nlp')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'stepik-dl-nlp'...\n",
            "remote: Enumerating objects: 66, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 256 (delta 35), reused 41 (delta 17), pack-reused 190\n",
            "Receiving objects: 100% (256/256), 42.16 MiB | 9.09 MiB/s, done.\n",
            "Resolving deltas: 100% (114/114), done.\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 1)) (0.22.1)\n",
            "Collecting spacy-udpipe\n",
            "  Downloading https://files.pythonhosted.org/packages/53/ea/dc89025c7f0ed8e6d7fd11893e53eddb53ecb20c017f8ffba4a11eea64ef/spacy_udpipe-0.1.0-py3-none-any.whl\n",
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 5)) (3.1.3)\n",
            "Collecting ipymarkup\n",
            "  Downloading https://files.pythonhosted.org/packages/d8/29/eaa1bcf649d6333dea829c05577c67f881d0555b6d77c1da72afda5c847d/ipymarkup-0.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 7)) (4.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 9)) (0.25.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 10)) (4.28.1)\n",
            "Collecting youtokentome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/16/4cb7a9358430996bd6fa7daf32421105fe37a7bd0e4da1f79496e15aa509/youtokentome-1.0.5-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 12)) (0.10.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 13)) (4.6.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 14)) (5.5.0)\n",
            "Collecting pyconll\n",
            "  Downloading https://files.pythonhosted.org/packages/2c/6e/c325d0db05ac1b8d45645de903e4ba691d419e861c915c3d4ebfcaf8ac25/pyconll-2.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.17.5)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.1.9)\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 22.3MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 22.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3)) (0.6.2)\n",
            "Collecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.4.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.6.1)\n",
            "Requirement already satisfied: intervaltree==2.1.0 in /usr/local/lib/python3.6/dist-packages (from ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2018.9)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome->-r stepik-dl-nlp/requirements.txt (line 11)) (7.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.5.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.3.4)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.3.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (45.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.4.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.1.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.5)\n",
            "Requirement already satisfied: requests>=2.21 in /usr/local/lib/python3.6/dist-packages (from pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2.21.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.1)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.9.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.6.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (7.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.2)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.2.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.12.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from intervaltree==2.1.0->ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.1.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (17.0.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.6.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.1.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (3.0.4)\n",
            "Building wheels for collected packages: ufal.udpipe\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp36-cp36m-linux_x86_64.whl size=5625850 sha256=53054cce7d1ba72260a7a21bc32dbdc2e7e4671b0a13d1098512d534673525fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "Successfully built ufal.udpipe\n",
            "Installing collected packages: ufal.udpipe, spacy-udpipe, pymorphy2-dicts, dawg-python, pymorphy2, ipymarkup, youtokentome, pyconll\n",
            "Successfully installed dawg-python-0.7.2 ipymarkup-0.5.0 pyconll-2.2.1 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 spacy-udpipe-0.1.0 ufal.udpipe-1.2.0.3 youtokentome-1.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:30.785285Z",
          "start_time": "2019-10-29T19:19:29.542846Z"
        },
        "id": "hDtjwsCqdQxO",
        "colab_type": "code",
        "outputId": "cf57c99f-3873-4cf3-8ab8-3dd0a07c8ce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import dlnlputils\n",
        "from dlnlputils.data import tokenize_corpus, build_vocabulary, texts_to_token_ids, \\\n",
        "    PaddedSequenceDataset, Embeddings\n",
        "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
        "from dlnlputils.visualization import plot_vectors\n",
        "\n",
        "init_random_seed()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_fukXOtdQxR",
        "colab_type": "text"
      },
      "source": [
        "## Загрузка данных и подготовка корпуса"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:31.270503Z",
          "start_time": "2019-10-29T19:19:30.787789Z"
        },
        "id": "sjXIFL9wdQxT",
        "colab_type": "code",
        "outputId": "9dd57f5f-996e-448e-e9bb-2430be5ccb15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "full_dataset = list(pd.read_csv('./stepik-dl-nlp/datasets/nyt-ingredients-snapshot-2015.csv')['input'].dropna())\n",
        "random.shuffle(full_dataset)\n",
        "\n",
        "TRAIN_VAL_SPLIT = int(len(full_dataset) * 0.7)\n",
        "train_source = full_dataset[:TRAIN_VAL_SPLIT]\n",
        "test_source = full_dataset[TRAIN_VAL_SPLIT:]\n",
        "print(\"Обучающая выборка\", len(train_source))\n",
        "print(\"Тестовая выборка\", len(test_source))\n",
        "print()\n",
        "print('\\n'.join(train_source[:10]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Обучающая выборка 125344\n",
            "Тестовая выборка 53719\n",
            "\n",
            "1/4 cup sour cream\n",
            "10 ounces swordfish, red snapper or other firm-fleshed fish\n",
            "1 tablespoon minced basil leaves\n",
            "Handful fresh parsley, finely minced\n",
            "4 ounces lard or butter, plus more for brushing tops\n",
            "4 to 5 green cardamom pods\n",
            "1 stick ( 1/4 pound) unsalted butter, softened\n",
            "1/4 teaspoon red pepper flakes, preferably Turkish or Aleppo (see note), more to taste\n",
            "1 tablespoon fresh lemon juice\n",
            "1/4 cup scallions, thinly sliced\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:32.137838Z",
          "start_time": "2019-10-29T19:19:31.272363Z"
        },
        "id": "KX1eACubdQxV",
        "colab_type": "code",
        "outputId": "a217bd10-6115-405a-a9cb-dbf03b645aba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "# токенизируем\n",
        "train_tokenized = tokenize_corpus(train_source)\n",
        "test_tokenized = tokenize_corpus(test_source)\n",
        "print('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sour cream\n",
            "ounces swordfish snapper other firm fleshed fish\n",
            "tablespoon minced basil leaves\n",
            "handful fresh parsley finely minced\n",
            "ounces lard butter plus more brushing tops\n",
            "green cardamom pods\n",
            "stick pound unsalted butter softened\n",
            "teaspoon pepper flakes preferably turkish aleppo note more taste\n",
            "tablespoon fresh lemon juice\n",
            "scallions thinly sliced\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:32.325205Z",
          "start_time": "2019-10-29T19:19:32.140837Z"
        },
        "id": "P6_WIaFFdQxY",
        "colab_type": "code",
        "outputId": "f992cb38-921a-41e7-8399-b931ec38cd6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# строим словарь\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=0.9, min_count=5, pad_word='<PAD>')\n",
        "print(\"Размер словаря\", len(vocabulary))\n",
        "print(list(vocabulary.items())[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Размер словаря 2267\n",
            "[('<PAD>', 0), ('tablespoons', 1), ('teaspoon', 2), ('chopped', 3), ('salt', 4), ('pepper', 5), ('cups', 6), ('ground', 7), ('fresh', 8), ('tablespoon', 9)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:32.686258Z",
          "start_time": "2019-10-29T19:19:32.327711Z"
        },
        "id": "gjMA9rtZdQxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# отображаем в номера токенов\n",
        "train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n",
        "test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n",
        "\n",
        "print('\\n'.join(' '.join(str(t) for t in sent)\n",
        "                for sent in train_token_ids[:10]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:32.967989Z",
          "start_time": "2019-10-29T19:19:32.688319Z"
        },
        "id": "9tKGp5NWdQxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist([len(s) for s in train_token_ids], bins=20);\n",
        "plt.title('Гистограмма длин предложений');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:33.001487Z",
          "start_time": "2019-10-29T19:19:32.970153Z"
        },
        "id": "-E2p0GxLdQxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SENTENCE_LEN = 20\n",
        "train_dataset = PaddedSequenceDataset(train_token_ids,\n",
        "                                      np.zeros(len(train_token_ids)),\n",
        "                                      out_len=MAX_SENTENCE_LEN)\n",
        "test_dataset = PaddedSequenceDataset(test_token_ids,\n",
        "                                     np.zeros(len(test_token_ids)),\n",
        "                                     out_len=MAX_SENTENCE_LEN)\n",
        "print(train_dataset[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxhMH1Y1dQxk",
        "colab_type": "text"
      },
      "source": [
        "## Алгоритм обучения - Skip Gram Negative Sampling\n",
        "\n",
        "**Skip Gram** - предсказываем соседние слова по центральному слову\n",
        "\n",
        "**Negative Sampling** - аппроксимация softmax\n",
        "\n",
        "$$ W, D \\in \\mathbb{R}^{Vocab \\times EmbSize} $$\n",
        "\n",
        "$$ \\sum_{CenterW_i} P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) \\rightarrow \\max_{W,D} $$\n",
        "\n",
        "$$ P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) = \\prod_j P(CtxW_j | CenterW_i; W, D) $$\n",
        "    \n",
        "$$ P(CtxW_j | CenterW_i; W, D) = \\frac{e^{w_i \\cdot d_j}} { \\sum_{j=1}^{|V|} e^{w_i \\cdot d_j}} = softmax \\simeq \\frac{e^{w_i \\cdot d_j^+}} { \\sum_{j=1}^{k} e^{w_i \\cdot d_j^-}}, \\quad k \\ll |V| $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:33.065376Z",
          "start_time": "2019-10-29T19:19:33.003081Z"
        },
        "id": "E9RpHAaJdQxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_diag_mask(size, radius):\n",
        "    \"\"\"Квадратная матрица размера Size x Size с двумя полосами ширины radius вдоль главной диагонали\"\"\"\n",
        "    idxs = torch.arange(size)\n",
        "    abs_idx_diff = (idxs.unsqueeze(0) - idxs.unsqueeze(1)).abs()\n",
        "    mask = ((abs_idx_diff <= radius) & (abs_idx_diff > 0)).float()\n",
        "    return mask\n",
        "\n",
        "make_diag_mask(10, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:33.101379Z",
          "start_time": "2019-10-29T19:19:33.068154Z"
        },
        "id": "9Qg7lq5VdQxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipGramNegativeSamplingTrainer(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, sentence_len, radius=5, negative_samples_n=5):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.negative_samples_n = negative_samples_n\n",
        "\n",
        "        self.center_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)\n",
        "        self.center_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n",
        "        self.center_emb.weight.data[0] = 0\n",
        "\n",
        "        self.context_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)        \n",
        "        self.context_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n",
        "        self.context_emb.weight.data[0] = 0\n",
        "\n",
        "        self.positive_sim_mask = make_diag_mask(sentence_len, radius)\n",
        "    \n",
        "    def forward(self, sentences):\n",
        "        \"\"\"sentences - Batch x MaxSentLength - идентификаторы токенов\"\"\"\n",
        "        batch_size = sentences.shape[0]\n",
        "        center_embeddings = self.center_emb(sentences)  # Batch x MaxSentLength x EmbSize\n",
        "\n",
        "        # оценить сходство с настоящими соседними словами\n",
        "        positive_context_embs = self.context_emb(sentences).permute(0, 2, 1)  # Batch x EmbSize x MaxSentLength\n",
        "        positive_sims = torch.bmm(center_embeddings, positive_context_embs)  # Batch x MaxSentLength x MaxSentLength\n",
        "        positive_probs = torch.sigmoid(positive_sims)\n",
        "\n",
        "        # увеличить оценку вероятности встретить эти пары слов вместе\n",
        "        positive_mask = self.positive_sim_mask.to(positive_sims.device)\n",
        "        positive_loss = F.binary_cross_entropy(positive_probs * positive_mask,\n",
        "                                               positive_mask.expand_as(positive_probs))\n",
        "\n",
        "        # выбрать случайные \"отрицательные\" слова\n",
        "        negative_words = torch.randint(1, self.vocab_size,\n",
        "                                       size=(batch_size, self.negative_samples_n),\n",
        "                                       device=sentences.device)  # Batch x NegSamplesN\n",
        "        negative_context_embs = self.context_emb(negative_words).permute(0, 2, 1)  # Batch x EmbSize x NegSamplesN\n",
        "        negative_sims = torch.bmm(center_embeddings, negative_context_embs)  # Batch x MaxSentLength x NegSamplesN\n",
        "        \n",
        "        # уменьшить оценку вероятность встретить эти пары слов вместе\n",
        "        negative_loss = F.binary_cross_entropy_with_logits(negative_sims,\n",
        "                                                           negative_sims.new_zeros(negative_sims.shape))\n",
        "\n",
        "        return positive_loss + negative_loss\n",
        "\n",
        "\n",
        "def no_loss(pred, target):\n",
        "    \"\"\"Фиктивная функция потерь - когда модель сама считает функцию потерь\"\"\"\n",
        "    return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgYDgowhdQxr",
        "colab_type": "text"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:33.130307Z",
          "start_time": "2019-10-29T19:19:33.103036Z"
        },
        "id": "9qeg9U48dQxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), 100, MAX_SENTENCE_LEN,\n",
        "                                          radius=5, negative_samples_n=25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.830221Z",
          "start_time": "2019-10-29T19:19:33.132062Z"
        },
        "scrolled": false,
        "id": "sc4EmNKOdQxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_val_loss, best_model = train_eval_loop(trainer,\n",
        "                                            train_dataset,\n",
        "                                            test_dataset,\n",
        "                                            no_loss,\n",
        "                                            lr=1e-2,\n",
        "                                            epoch_n=2,\n",
        "                                            batch_size=8,\n",
        "                                            device='cpu',\n",
        "                                            early_stopping_patience=10,\n",
        "                                            max_batches_per_epoch_train=2000,\n",
        "                                            max_batches_per_epoch_val=len(test_dataset),\n",
        "                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=1, verbose=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.862018Z",
          "start_time": "2019-10-29T19:20:12.832046Z"
        },
        "id": "pjkpq7A-dQxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "torch.save(trainer.state_dict(), 'models/sgns.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.888270Z",
          "start_time": "2019-10-29T19:20:12.864706Z"
        },
        "id": "pzOe1hWhdQxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "trainer.load_state_dict(torch.load('models/sgns.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl6rqirEdQx5",
        "colab_type": "text"
      },
      "source": [
        "## Исследуем характеристики полученных векторов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.919904Z",
          "start_time": "2019-10-29T19:20:12.890671Z"
        },
        "id": "yCPcIhK2dQx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.942708Z",
          "start_time": "2019-10-29T19:20:12.921619Z"
        },
        "id": "PTCal974dQx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings.most_similar('chicken')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.965936Z",
          "start_time": "2019-10-29T19:20:12.944423Z"
        },
        "id": "GVTyoo1kdQx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings.analogy('cake', 'cacao', 'cheese')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.991060Z",
          "start_time": "2019-10-29T19:20:12.967532Z"
        },
        "id": "P8ngxFWjdQyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_words = ['salad', 'fish', 'salmon', 'sauvignon', 'beef', 'pork', 'steak', 'beer', 'cake', 'coffee', 'sausage', 'wine', 'merlot', 'zinfandel', 'trout', 'chardonnay', 'champagne', 'cacao']\n",
        "test_vectors = embeddings.get_vectors(*test_words)\n",
        "print(test_vectors.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:13.318676Z",
          "start_time": "2019-10-29T19:20:12.996595Z"
        },
        "id": "3FDLsLgbdQyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches((10, 10))\n",
        "plot_vectors(test_vectors, test_words, how='svd', ax=ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no-Xifv1dQyG",
        "colab_type": "text"
      },
      "source": [
        "## Обучение Word2Vec с помощью Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:13.613797Z",
          "start_time": "2019-10-29T19:20:13.321353Z"
        },
        "id": "a1ka1QXvdQyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.075005Z",
          "start_time": "2019-10-29T19:20:13.615729Z"
        },
        "id": "eqvZsIMJdQyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vec = gensim.models.Word2Vec(sentences=train_tokenized, size=100,\n",
        "                                  window=5, min_count=5, workers=4,\n",
        "                                  sg=1, iter=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.109583Z",
          "start_time": "2019-10-29T19:20:17.076599Z"
        },
        "id": "4UnEIhdfdQyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vec.wv.most_similar('chicken')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.176357Z",
          "start_time": "2019-10-29T19:20:17.112948Z"
        },
        "id": "reTeCzkmdQyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gensim_words = [w for w in test_words if w in word2vec.wv.vocab]\n",
        "gensim_vectors = np.stack([word2vec.wv[w] for w in gensim_words])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.428874Z",
          "start_time": "2019-10-29T19:20:17.179311Z"
        },
        "id": "hjlouEkNdQyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches((10, 10))\n",
        "plot_vectors(gensim_vectors, test_words, how='svd', ax=ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaqakPJ0dQyU",
        "colab_type": "text"
      },
      "source": [
        "## Загрузка предобученного Word2Vec\n",
        "\n",
        "Источники готовых векторов:\n",
        "\n",
        "https://rusvectores.org/ru/ - для русского языка\n",
        "\n",
        "https://wikipedia2vec.github.io/wikipedia2vec/pretrained/ - много разных языков"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.460133Z",
          "start_time": "2019-10-29T19:20:17.430563Z"
        },
        "id": "MXpuVeoFdQyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim.downloader as api"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.980509Z",
          "start_time": "2019-10-29T19:20:17.462239Z"
        },
        "id": "WChbrRiCdQyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "available_models = api.info()['models'].keys()\n",
        "print('\\n'.join(available_models))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.649035Z",
          "start_time": "2019-10-29T19:20:17.984118Z"
        },
        "scrolled": false,
        "id": "JdII2xXjdQyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained = api.load('word2vec-google-news-300')  # > 1.5 GB!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.651388Z",
          "start_time": "2019-10-29T19:19:29.817Z"
        },
        "id": "DoHbYzQvdQyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained.most_similar('cheese')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.652649Z",
          "start_time": "2019-10-29T19:19:29.820Z"
        },
        "id": "KPNfvXF8dQyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained.most_similar(positive=['man', 'queen'], negative=['king'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.653584Z",
          "start_time": "2019-10-29T19:19:29.823Z"
        },
        "id": "y5tYQbtEdQyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_words = [w for w in test_words if w in pretrained.vocab]\n",
        "pretrained_vectors = np.stack([pretrained[w] for w in pretrained_words])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.654594Z",
          "start_time": "2019-10-29T19:19:29.828Z"
        },
        "id": "BSF5P6QedQyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches((10, 10))\n",
        "plot_vectors(pretrained_vectors, test_words, how='svd', ax=ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cga4I3oWdQyp",
        "colab_type": "text"
      },
      "source": [
        "## Заключение\n",
        "\n",
        "* Реализовали Skip Gram Negative Sampling на PyTorch\n",
        "* Обучили на корпусе рецептов\n",
        "    * Сходство слов модель выучила неплохо\n",
        "    * Для аналогий мало данных\n",
        "* Обучили SGNS с помощью библиотеки Gensim\n",
        "* Загрузили веса Word2Vec, полученные с помощью большого корпуса (GoogleNews)\n",
        "    * Списки похожих слов отличаются!\n",
        "    * Аналогии работают"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfGrLms0dQyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}